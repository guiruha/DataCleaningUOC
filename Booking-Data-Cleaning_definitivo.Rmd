---
title: 'Práctica 2: Booking Data Cleaning'
author: "Gerard Alcalde and Guillem Rochina"
date: "2022-12-24"
output:
  word_document: default
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
# Importamos todas las librerias a utilizar
knitr::opts_chunk$set(echo = TRUE)
options(tinytex.verbose = TRUE)
suppressWarnings(library(dplyr))
suppressWarnings(library(ggplot2))
suppressWarnings(library(tidyr))
suppressWarnings(library(lubridate))
suppressWarnings(library(stringr))
suppressWarnings(library(knitr))
suppressWarnings(library(VIM))
suppressWarnings(library(corrplot))
suppressWarnings(library(MASS))
suppressWarnings(library(devtools))
install_github("ProcessMiner/nlcor")
suppressWarnings(library(nlcor))
```

## 1. DESCRIPCIÓN DEL DATASET

### ¿Por qué es importante y qué pregunta/problema pretende responder?

#### 1.1. Descripción general

El conjunto de datos ha tratar es el obtenido como resultado de la práctica anterior de Web Scraping. El dataset presenta los datos e indicadores más relevantes de cada uno de los hoteles encontrados en función de determinados criterios de búsqueda (ciudad, fecha de check-in, fecha de check-out, número de adultos, niños y habitaciones). En este dataset en concreto presentamos datos para distintas fechas (diciembre, marzo y junio), ciudades (barcelona, madrid, valencia), número de adultos, niños y habitaciones a fin de tener una muestra más amplia e informativa que la que nos daría una búsqueda con tan sólo unos parámetros fijos.

A continuación, se realiza una breve descripción de cada una de las variables disponibles antes de realizar la limpieza:

* **Name (str)**: Nombre del hotel.
* **City (str)**: Nombre de la ciudad donde se realiza la búsqueda.
* **Check-in (str)**: Fecha de entrada al hotel.
* **Check-out (str)**: Fecha de salida del hotel.
* **Adults (int)**: Nº de adultos para los que se realiza la reserva.
* **Num_rooms (int)**: Nº de habitaciones reservadas.
* **Address (str)**: Dirección postal del hotel, nombre de calle, barrio de la ciudad, código postal, etc.
* **Hotel_coordinates (str)**: Latitud y longitud de la ubicación del hotel.
* **Hotel_score (int)**: Nota general que recibe el hotel por los usuarios.
* **Hotel_scores (dict)**: Puntuaciones de cada una de las dimensiones que puede valorar un usuario.
* **Hotel description (list)**: Descripción aportada por el propietario del hotel
* **Features (list)**: Lista de servicios añadidos del hotel.
* **Room_data (dict)**: Conjunto de diccionarios que recogen las características de cada una de las habitaciones disponibles en el hotel.
* **Page_count (int)**: Posición en la que ha aparecido el hotel en el buscador.
* **Current_page (int)**: Página en la que ha aparecido el hotel en el buscador.

#### 1.2. Importancia y problema a responder

Aparecer en los primeros resultados de una búsqueda de un usuario por internet se ha convertido en los últimos años en una de las preocupaciones principales de cualquier negocio que busca hacerse un hueco en el mundo digital. Este fenómeno explica la proliferación de herramientas como el SEO y el SEM, mediante las cuales se intenta innovar (más allá del pago de tarifas para aparecer en las primeras páginas de búsqueda) a fin de lograr un mejor “posicionamiento” en buscadores como Google o Bing. El negocio hotelero no es una excepción en este caso.
No obstante, el uso de dichas herramientas no resulta útil si nos enfrentamos a un buscador dentro de una página web, como el caso que nos ocupa con Booking.com. ¿Qué hacer entonces? ¿Cómo mejorar el posicionamiento dentro de la web?
Asimismo, la fijación de precios o “pricing” puede ser una tardea ardua en los últimos años, pues la aparición de apartamentos vacacionales a través de plataformas como AirBnB ha incrementado de manera significativa la competencia a la que se deben enfrentar los propietarios de los hoteles, lo que influye en última instancia los precios a establecer por sus servicios. Dicho esto, ¿Qué precios se deben fijar dadas las características de un hotel? ¿Añadir un servicio adicional permitiría a un hotel elevar los precios de sus habitaciones?.
Ante las preguntas presentadas en los párrafos anteriores, el objetivo de este proyecto es dual. Por un lado se busca obtener las características que hacen que un hotel esté mejor posicionado que otro en el metabuscador de Booking.com y, por el otro, se persigue desarrollar un modelo que indique qué precios se deberían establecer dadas las características de una habitación y qué elementos pueden incrementar el precio de forma relevante.
En conclusión, el presente proyecto pretende dar continuidad a la Práctica 1, explotando los datos que se obtuvieron mediante web scrapping para ayudar a superar dos de los desafíos más relevantes para el sector hotelero.


## 2. LIMPIEZA DE LOS DATOS

Dado que muchas de las variables que desecharemos en la sección de selección e integración las vamos a utilizar en a continuación, procedemos a realizar previamente la limpieza de los datos. A fin de iniciar el proceso, en primer lugar, se debe realizar la lectura del fichero csv que obtuvimos en la práctica anterior. Como resultado de la función de R-base “read.csv” obtenemos un objeto data.frame, que manipularemos en la presente sección.

```{r}
# Lectura y breve descripción de los datos

booking <- read.csv("hotels_data.csv", header = TRUE)
summary(booking)
```

Tanto con la función “summary” como con la función “class” nos muestran que a cada variable se le ha asignado o bien la clase de entero o la de string. En concreto, muchas de las variables interpretadas como string se tratan de diccionarios y listas, factor que deberemos tener en cuenta a la hora de trabajar con la limpieza de estas variables.

```{r}
# Tipo de dato asignado a cada variable

res <- sapply(booking, function(x) class(x))
kable(data.frame(var=names(res), clase=as.vector(res)))
```

#### 2.1. Transformación de las variables en el formato adecuado

Como se ha comentado, existen algunas variables que presentan todavía una estructura no válida para tratarlos estadísticamente o introducirlos en un modelo de data mining. En esta sección nos encargamos de tratar dichas columnas a fin de obtener nuevas variables con un formato adecuado, eliminando a su vez cualquier información que no nos sea de utilidad.

La columna hotel_coordinates incluye tanto los valores de latitud como los de longitud, separados por una coma. Dado que ambos dos valores nos indican información distinta, pues la latitud nos proporcionar información de la posición en dirección norte o sur del ecuador y la longitud información de la posición en dirección este u oeste, en este caso hemos optado por separarlos en dos columnas distintas.

```{r}
# Separamos los valores de la variable hotel coordinates en dos nuevas columnas, latitud y longitud
# La columna hotel_coordinates se elimina en el proceso

booking <- booking %>%
  separate(hotel_coordinates, c("latitude", "longitude"), sep = ",", remove = FALSE)

booking$latitude <- as.numeric(booking$latitude)
booking$longitude <- as.numeric(booking$longitude)
```

Por su parte, hotel_scores alberga diccionarios en cada uno de los registros. Cada diccionario consta del nombre de cada dimensión (característica a valorar) como clave y la valoración que recibe como valor. Dado que R lo ha interpretado como un string, nuestro enfoque en su limpieza se ha basado en separar la información en base a las comas como separador y, una vez obtenidas todas las columnas resultantes, extraer los datos numéricos de cada registro para quedarnos solo con los scores de cada dimensión con la función “parse_numbers”.

```{r}
# Definimos un vector con los nombres de las columnas a crear

columns <-  c("staff_score", "facilities_score", "cleanliness_score", "comfort_score", "value_for_money_score", "location_score", "free_wifi_score")

# Separamos los strings (diccionarios) en función de las comas y creamos las columnas correspondientes. La primera columna creada es una NA porque todos los primeros registros en cada diccionario están vacíos (por un fallo en la extracción de datos). Estableciendo la columna como NA, indicamos a la función separate que ignore dichos datos y no cree columna alguna para ellos.

booking <- booking %>%
  separate(hotel_scores, c(NA, "staff_score", "facilities_score", "cleanliness_score", "comfort_score", "value_for_money_score", "location_score", "free_wifi_score"), sep = ",", remove = FALSE)

# Extraemos los valores numéricos de cada columna con la función del paquete readr, "parse_number"

booking[columns] <- apply(booking[columns], 2, readr::parse_number)
```

Dado que de las fechas tan sólo nos interesa el mes en el que se realiza la reserva (un alojamiento con unas características determinadas podría estar mejor posicionado en un determinado mes que en otro o tener un precio distinto) procedemos a extraer dicha información de las variables check-in y check-out. De hecho, dado que los datos que hemos extraído corresponden al mismo mes en todos los registros, tan sólo nos quedaremos con una columna de mes. Asimismo, no crearemos una columna is_june, pues la dejaremos como caso base, es decir, cuando no sea ninguna de las otras dos opciones (o Diciembre o Marzo).

```{r}
booking <- booking %>%
  # Separamos el nombre del mes del resto de elementos del registro para check_in y check_out. Eliminamos las columnas en el proceso.
  separate(check.in, c(NA, "month", NA), sep = "-", remove = FALSE) %>%
  mutate(is_december = ifelse(month == "December", 1, 0),
         is_march = ifelse(month == "March", 1, 0))
```

De la columna address tan sólo nos interesa el código postal (el barrio no se incluye en todos los registros a diferencia del CP, así que hemos optado por quedarnos con estos valores), por lo que extraemos dicha información para crear una nueva columna llamada postal_code.

```{r}
# Hemos tenido que cambiar esta función para que el extract no diera problemas
Sys.setlocale('LC_ALL', 'C')
# Extraemos los valores de codigo postal de la columna addres utilizando expressiones regulares
booking <- booking %>%
  extract(address, c("postal_code"), regex = "( [0-9]{5} )", remove = FALSE)

```

Los features, entendidos como servicios adicionales más allá de la propia habitación, fueron guardados como una lista. No obstante, de nuevo nos encontramos como R lo ha identificado como string. Es por ello por lo que realizamos un tratamiento similar al de la columna de scores. En este caso creamos tantas variables dummy (columnas dicotómicas con valores de 1, en caso de que el feature exista en dicho registro, o 0, en caso de que no exista) como servicios adicionales de hotel consideramos interesantes.

```{r}
# Definimos las features que consideramos que son interesantes. Utilizamos este formato para filtrar por expresión regular.
feature_list <- c("('Free WiFi')", "('Air conditioning')", "('24-hour front desk')", "('Safe')", "('Heating')", "('Elevator')", "('Private Bathroom')", "('Non-smoking rooms')", "('Aparments')", "('City view')","('Kitchen')", "('Pet friendly')", "('Swimming pool')", "('Balcony')")

# Creamos un bucle en el que se recorre cada elemento de la lista de features
for (feature in feature_list){
  # Se eliminan los parentésis y comillas de la variable local feature para crear el nombre de la columna.
  col_name <- str_replace(str_replace(str_to_lower(str_extract(feature, "([A-Z][a-z]*( |-)?[A-Z]?[a-z]* ? ?[A-Z]?[a-z]*)")), " ", "_"), "-", "_")
  booking <- booking %>%
  # Extraemos el nombre del feature de cada uno de los registros (strings). En caso de que no encuentre ningún valor devuelve un NA.
  extract(features, c(col_name), regex = feature, remove = FALSE) %>%
  # Transformamos la columna que acaba de ser creada para que indique con un 1 si el registro tenia dicho servicio y 0 si el valor era NA (no tenida dicho servicio)
  mutate_(.dots = setNames(list(paste0("as.integer(!is.na(",col_name,"))")), col_name))
}

```

Por otro lado, nos encontramos con la columna que incluye la descripción del hotel, esta descripción es muy amplia y un análisis profundo de ella requeriría de técnicas de NLP, las cuales no son el objetivo de esta práctica. No obstante, consideramos que la longitud de la descripción del hotel sí que puede tener una relación con la posición en el buscador, por lo que transformaremos esta variable a una nueva variable que contenga el número total de palabras que contiene la descripción.

```{r}
# Evaluamos la longitud de la descripción del hotel
booking$length_description <- lengths(gregexpr("\\W+", booking$hotel_description)) + 1

```

Finalmente, en la columna room_data, se incluye mucha información referente a los tipos de habitaciones disponibles, precios, características, etc. Esta se había extraído de esta forma con el objetivo de procesarla mediante un diccionario de Python. No obstante, al haber optado por un procesado con R este se complicará un poco más. En esta columna tenemos distintos datos que pueden ser muy relevantes, de los cuales extraeremos los siguientes:
•	min_price: Precio de la habitación más económica.
•	max_price: Precio de las habitaciones más caras del hotel.
•	suite_available: Una de las habitaciones es de tipo suite.
•	is_apartment: Una de las habitaciones es de tipo apartamento.
•	free_cancelation_available: Alguna de las habitaciones tiene cancelación gratuita.

```{r}
# Función que encuentra el precio mínimo de la habitación dado un string con la información de las habitaciones
find_min_price <- function(text) {
  if (text=="{}"){
    return(NA)
  }
  a <- gregexpr("([0-9]*', 'room_capacity)", text)
  
  min_price <- NA
  for (value in a[[1]]){
    substring <- substr(text, value, value+20)
    price <- as.numeric(strsplit(substring, "'")[[1]][1])
    if (is.na(min_price)) {
      min_price <- price
    }
    
    if (price < min_price) {
      min_price <- price
    } 
  }
  if (min_price == 0){
    return(NA)
  }
  else {
    return(min_price)
  }
}

# Aplicamos la función a todos los valores
min_price_vector <- c()
for (i in seq(1, length(booking$room_data))) {
  min_price_vector <- append(min_price_vector, find_min_price(booking$room_data[i]))
}

# Guardamos los resultados en una nueva columna.
booking$min_price <- min_price_vector

# Función que encuentra el precio máximo de la habitación dado un string con la información de las habitaciones
find_max_price <- function(text) {
  if (text=="{}"){
    return(NA)
  }
  a <- gregexpr("([0-9]*', 'room_capacity)", text)
  
  max_price <- NA
  for (value in a[[1]]){
    substring <- substr(text, value, value+20)
    price <- as.numeric(strsplit(substring, "'")[[1]][1])
    if (is.na(max_price)) {
      max_price <- price
    }
    
    if (price > max_price) {
      max_price <- price
    } 
  }
  if (max_price == 0){
    return(NA)
  }
  else {
    return(max_price)
  }
}

# Aplicamos la función a todos los valores
max_price_vector <- c()
for (i in seq(1, length(booking$room_data))) {
  max_price_vector <- append(max_price_vector, find_max_price(booking$room_data[i]))
}

# Guardamos los resultados en una nueva columna.
booking$max_price <- max_price_vector

# Buscamos si hay una habitación en suite
is_suite_vector <- c()
for (i in seq(1, length(booking$room_data))) {
  is_suite <- gregexpr("(suite)", booking$room_data[i])[[1]][1]
  
  if (is_suite==-1) {
    is_suite_vector <- append(is_suite_vector, 0)
  } else {
    is_suite_vector <- append(is_suite_vector, 1)
  }
}

# Guardamos los resultados en una nueva columna.
booking$is_suite <- is_suite_vector

# Buscamos si hay opción de apartamento
is_apartment_vector <- c()
for (i in seq(1, length(booking$room_data))) {
  is_apartment <- gregexpr("(Apartment)", booking$room_data[i])[[1]][1]
  
  if (is_apartment==-1) {
    is_apartment_vector <- append(is_apartment_vector, 0)
  } else {
    is_apartment_vector <- append(is_apartment_vector, 1)
  }
}

# Guardamos los resultados en una nueva columna.
booking$is_apartment <- is_apartment_vector

# Buscamos si tiene cancelación gratuita
free_cancelation_vector <- c()
for (i in seq(1, length(booking$room_data))) {
  free_cancelation <- gregexpr("(Free cancellation)", booking$room_data[i])[[1]][1]
  
  if (free_cancelation==-1) {
    free_cancelation_vector <- append(free_cancelation_vector, 0)
  } else {
    free_cancelation_vector <- append(free_cancelation_vector, 1)
  }
}

# Guardamos los resultados en una nueva columna.
booking$has_free_cancelation <- free_cancelation_vector

```


#### 2.2. Ceros y elementos vacíos

Dando paso a el tratamiento de ceros y elementos vacios, lo primero que debemos hacer es identificar aquellas variables en las que encontramos valores faltantes. Como observamos, "postal_code" (19), "longitude" (16), "hotel_score" (8), min (45) y max_price (17) y el resto de variables relacionadas con los scores (60) presentan valores faltantes, destacando free_wifi_score con 423 registros faltantes, consecuencia con toda seguridad de la ausencia de este servicio en dichos hoteles.

```{r}
# Comprobamos que columnas tienen datos faltantes
kable(sapply(booking, function(y) sum(is.na(y))))
  
```

Por lo que respecta a la variable "postal_code" encontramos que en todos aquellos registros donde se encuentan NAs, también hallamos datos faltantes en la mayoría del resto de columnas. Por tanto, dado que dichos registros no nos aportan información de valor, procedemos a eliminarlos del dataset.

```{r}

# Comprobamos algunos de los registros donde hay datos faltantes de postal_code
kable(booking[is.na(booking$postal_code),] %>%
  dplyr::select(c("city", "month", "postal_code", "longitude", "hotel_score", "staff_score")) %>%
  head(3))

# Filtramos los datos para eliminar aquellos registros que tienen NAs en postal_code
booking <- booking[!is.na(booking$postal_code),]
  
```

Si comprobamos de nuevo que columnas presentan todavía datos faltantes, observamos que en algunos casos (como "longitude") hemos logrado eliminar todos los datos faltantes, mientras que en el resto se han reducido en 16 unidades el número de registros con NAs, pues como se había comentado todos aquellos registros con ausencia de postal_code, también carecía del resto de datos.

```{r}
# Comprobamos que columnas tienen datos faltantes
kable(sapply(booking, function(y) sum(is.na(y))))
```

A continuación, analizamos los datos faltantes de hotel_score. En este caso, encontramos que la ausencia de datos se registra de dos formas distintas, o bien con la introducción de NAs o bien con el valor -1. Por tanto, dado que el tratamiento en ambos casos va a ser el mismo, transformamos todos los valores -1 en NAs para poder llevar a cabo el siguiente proceso de inputación.

```{r}

# Comprobamos algunos de los registros donde hay datos faltantes de hotel_score
booking[is.na(booking$hotel_score),] %>%
  dplyr::select(c("city", "month", "hotel_score", "staff_score", "location_score", "longitude", "facilities_score", "cleanliness_score", "comfort_score")) %>%
  head(5)

```

```{r}

# Comprobamos algunos de los registros donde hay datos faltantes de hotel_score
kable(booking[booking$hotel_score == "-1",] %>%
  dplyr::select(c("city", "month", "hotel_score", "staff_score", "location_score", "longitude", "facilities_score", "cleanliness_score", "comfort_score")) %>%
  tail(5))

kable(booking[booking$hotel_score == "-1",] %>%
  dplyr::select(c("city", "month", "hotel_score", "staff_score", "free_wifi_score", "free_wifi")) %>%
  tail(5))


# Tranformamos los valores -1 en NAs para poder llevar a cabo el proceso de inputación correctamente.
booking$hotel_score <- ifelse(booking$hotel_score == "-1", NA, booking$hotel_score)

```

Dado que en este caso tan sólo encontramos datos faltantes en las columnas de score no consideramos eliminar estos registros, sino llevar a cabo una imputación. Aunque se podría haber utilizado una metología más simple como el uso de la media o la mediana, hemos optado por hacer uso de un método más complejo, basado en la similitud entre registros, en concreto llevar a cabo una imputación mediante el algoritmo de K vecinos más próximos. En este caso, la función se encarga de devolver los mismos valores en caso de que ya existan en el dataset y el valor de los registros más "similares" en el caso de un registro con NAs.

```{r}

# Imputamos los datos con K Neirest Neighbours
booking$hotel_score <- kNN(booking)$hotel_score
booking$staff_score <- kNN(booking)$staff_score 
booking$facilities_score <- kNN(booking)$facilities_score
booking$cleanliness_score <- kNN(booking)$cleanliness_score
booking$comfort_score <- kNN(booking)$comfort_score
booking$value_for_money_score <- kNN(booking)$value_for_money_score
booking$location_score <- kNN(booking)$location_score
booking$free_wifi_score <- kNN(booking)$free_wifi_score

```

En el caso de los precios, encontramos NAs en aquellos registros que presentaban valores iguales a 0 (no son valores lógicos). En este caso también encontramos que los únicos valores faltantes en estos registros corresponde a precio, por lo que procedemos de la misma forma que con los scores.

```{r}

kable(booking[is.na(booking$min_price),] %>%
  dplyr::select(c("city", "month", "hotel_score", "max_price", "min_price")) %>%
  tail(5))

```

```{r}

# Imputamos los datos con K Neirest Neighbours
booking$max_price <- kNN(booking)$max_price
booking$min_price <- kNN(booking)$min_price 

```

Una última comprobación nos revela que el dataset ya no cuenta con datos faltantes, con independencia del formato que pudieran presentar (strings vacías, valores de -1 en integers, o simplemente NAs).

```{r}
# Comprobamos que columnas tienen datos faltantes (si los hay)
kable(sapply(booking, function(y) sum(is.na(y))))
```

```{r}
#Comprobamos que columnas tienen datos faltantes (si los hay)
kable(sapply(booking, function(y) sum(y == "")))
```

#### 2.3. Valores extremos

Los outliers son aquellos datos extremos que dada su distancia con respecto al grueso de la distribución a priori resultan no ser congruentes si los comparamos con la población/muestra analizada. Existen varias vías para detectarlos, no obstante el método más rápdio y común se basa en el uso de boxplots, donde se detectan aquellos valores que se sitúan 1.5 veces más allá del rango intercuartílico con respecto a la mediana. Asimismo, en este trabajo también haremos uso de la función "boxplots.stats()" a fin de detectar, con un output más allá del visual, los outliers que encontramos usando boxplots. Dado que muchas de las variables que hemos generado en la fase de limpieza son dicotómicas, tan solo nos encargaremos de comprobar si existen valores extremos en las columnas de **num_rooms**, **latitude**, **longitude**, **hotel_score** y el resto de **scores del hotel**, **length_description**, **min_price** y **max_price**.


```{r}

# Graficamos box-plot con ggplot
ggplot(booking, aes(y = num_rooms, color = city))+
  geom_boxplot() +
  facet_grid(~city)

# Sacamos los valores considerados outliers
boxplot.stats(booking$num_rooms)$out

```

Empezamos analizando el número de habitaciones y observamos que en las 3 ciudades el número oscila entre 2 y 3 habitaciones para las búsquedas realizadas, lo cual son números razonables, aunque en Valencia esta aparezca como un outlier. Por lo tanto, aceptamos los valores outliers observados en la ciudad de Valencia como valores correctos.

```{r}
# Graficamos box-plot con ggplot
ggplot(booking, aes(y = latitude, color = city))+
  geom_boxplot() +
  facet_grid(~city)

# Sacamos los valores considerados outliers
boxplot.stats(booking$latitude)$out

```

Observamos que la latitud contiene valores distintos para cada ciudad, como es de esperar al tener cada una una ubicación distinta. En el caso de Valencia vemos una mayor dispersión y tenemos la presencia de algunos outliers, esto es debido a que probablemente, al ser una ciudad más pequeña que Madrid o Barcelona tenga una menor disponibilidad hotelera y algunos de los hoteles hayados no pertenezcan a Valencia ciudad sino a los alrededores de esta.

```{r}
# Graficamos box-plot con ggplot
ggplot(booking, aes(y = longitude, color = city))+
  geom_boxplot() +
  facet_grid(~city)

# Sacamos los valores considerados outliers
boxplot.stats(booking$longitude)$out

```

Para la longitud obtenemos unos resultados parecidos, pues hay una menor dispersión en los outliers de Valencia. Dado que estas variaciones tan pequeñas en los resultados son razonables, aceptamos los resultados como validos.

```{r}
# Graficamos box-plot con ggplot
ggplot(booking, aes(y = hotel_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

# Sacamos los valores considerados outliers
boxplot.stats(booking$hotel_score)$out

```

En cuanto hotel_score, por un lado observamos que la puntuación general del hotel oscila entre el 8 y el 9 siendo mayor en Madrid que en Barcelona o Valencia. También nos encontramos que hay outliers en las 3 ciudades no obstante todos se hayan en una puntuación entre el 5 y el 10, cuando el rango factible es entre 0 y 10, por lo que pueden ser valores anómalos de hoteles que han obtenido una puntuación más baja de lo normal, pero no por ello deben ser descartados. El análisis de esta variable ser presenta a modo de demostración general:

```{r}

# Seleccionamos muestras de habitaciones con nota menor a 7
kable(booking %>%
  filter(hotel_score <= 7.0) %>%
  dplyr::select(c("hotel_score", "facilities_score", "cleanliness_score", "location_score")) %>%
  sample_n(5))

```

Analizando el resto de scores, se observa que los hoteles con una mala puntuación lo hacen en la mayoría de sus características, lo que corrobora los resultados obtenidos y no es una sección que ha penalizado a los resultados obtenidos.

```{r}
# Seleccionamos muestras de habitaciones con nota de 10
kable(booking %>%
  filter(hotel_score == 10.0) %>%
  dplyr::select(c("hotel_score", "facilities_score", "cleanliness_score", "location_score") %>%
  sample_n(3))
```

Los hoteles con una puntuación de 10 destacan por obtener esta puntuación en todos los aspectos y tienen unas descripciones relativamente cortas, los precios mínimos y los máximos son muy distantes lo que sorprende. Dado que solo corresponde a 3 registros y que los resultados son razonables, puede corresponder a un hotel nuevo con muy pocas valoraciones, lo que permitiría una nota excepcionalmente alta otorgada por unos pocos clientes.

```{r}
# Graficamos box-plot con ggplot
ggplot(booking, aes(y = facilities_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

# Sacamos los valores considerados outliers
boxplot.stats(booking$facilities_score)$out
```

Vemos que los resultados obtenidos continuan en la línea anterior con algunos outliers en Barcelona o Valencia, pero con resultados lógicos.

A continuación evaluamos aquellos hoteles que han obtenido una nota inferior al 6.5 para identificar si se trata de un error o no.

```{r}

# Seleccionamos una muestra de hoteles con facilities_score menor a 6.5
kable(booking %>%
  filter(facilities_score <= 6.5) %>%
  dplyr::select(c("hotel_score", "facilities_score", "cleanliness_score", "location_score") %>%
  sample_n(3))

```

Observamos que los hoteles parecen tener unas características consistentes y que la extracción de características ha sido correcta por lo que no podemos atribuir los resultados a un error sino meramente a una baja calidad del hotel.

```{r}

ggplot(booking, aes(y = staff_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$staff_score)$out

```

Para la puntuación otorgada a los trabajadores del hotel obtenemos unos resultados similares a los previos, con la diferencia de que en este apartado la puntuación promedio es más elevada rondando el 9 en las 3 ciudades y todas presentan outliers con valores inferiores.

Evaluamos aquellos resultados con puntuaciones muy bajas.

```{r}

booking %>%
  filter(staff_score <= 6.7) %>%
  sample_n(3)

```

De los resultados obtenidos sorprende uno en Valencia que tiene unas puntuaciones muy elevadas excepto en **staff_score** la cual es bastante baja. Si nos fijamos estas puntuaciones son muy exactas (10.0, 9.0, 7.5 o 5.0) lo cual puede venir dado por un número de puntuaciones muy reducidas en las que un cliente ha puntuado de forma muy negativa este aspecto. No por ello eliminaremos el registro o lo modificaremos ya que es razonable el resultado, y afectará en la posición que presenta el hotel en el buscador.

A continuación se evalúa la limpieza del hotel.

```{r}

ggplot(booking, aes(y = cleanliness_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$cleanliness_score)$out

```

Observamos unos resultados muy similares a los anteriores. Para descartar algún error observamos aquellos registros con puntuaciones excepcionalmente bajas.

```{r}

booking %>%
  filter(cleanliness_score < 7.0) %>%
  sample_n(3)

```

Vemos un comportamiento razonable en estos hoteles, por lo que, siendo resultados razonables, aceptamos estos valores outliers.

A continuación, se valora la puntuación de la comodidad.

```{r}

ggplot(booking, aes(y = comfort_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$comfort_score)$out

```

```{r}

booking %>%
  filter(comfort_score < 7.0) %>%
  sample_n(3)

```

```{r}

ggplot(booking, aes(y = value_for_money_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$value_for_money_score)$out

```

```{r}

booking %>%
  filter(value_for_money_score <= 7.0) %>%
  sample_n(3)

```

```{r}

ggplot(booking, aes(y = location_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$location_score)$out

```

```{r}

booking %>%
  filter(location_score < 7.0) %>%
  sample_n(3)

```

```{r}

ggplot(booking, aes(y = free_wifi_score, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$free_wifi_score)$out

```

```{r}

booking %>%
  filter(free_wifi_score < 5.0 & free_wifi_score > 0) %>%
  sample_n(3)

```


A continuación se evalúa la longitud de las descripciones.

```{r}

ggplot(booking, aes(y = length_description, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$length_description)$out

```


Observamos que las descripciones tienen una longitud muy similar en las 3 ciudades en torno a las 120-170 palabras, presentando algún outlier por la parte superior, pero siempre con descripciones inferiores a las 400 palabras. Aunque corresponda a una descripción un poco extensa, no parece que corresponda a un outlier sino simplemente a una descripción más extensa de lo habitual.

A continuación evaluaremos el precio mínimo de las habitaciones del hotel.

```{r}

ggplot(booking, aes(y = min_price, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$min_price)$out

```

Observamos que el precio mínimo tiene una gran disparidad oscilando de promedio entre los 100€ y los 500€, siendo Valencia la ciudad con el menor rango intercuartil con presencia de outliers en el rango superior de precios, con alguna habitación con precios cercanos a los 1000€, que aunque puedan parecer caros, son razonables en algún hotel de lujo de la ciudad.

A continuación se evalúan los precios máximos.

```{r}

ggplot(booking, aes(y = max_price, color = city))+
  geom_boxplot() +
  facet_grid(~city)

boxplot.stats(booking$max_price)$out

```

Observamos unos precios promedio mucho más elevados con un mayor rango intercuartil en la ciudad de Valencia, lo que sorprende al ser lo contrario que con los precios mínimos. También se observa que los outliers en este caso aparecen por el lado inferior especialmente en la ciudad de Madrid, aunque con precios positivos, que podrían corresponder a hoteles muy económicos o albergues.

Por otro lado sorprende ver que los precios máximos no superan el valor máximo observado en el precio mínimo, lo que se puede atribuir a que aquellos hoteles con precios muy elevados no ofrecían una variedad de habitaciones y precios.


## 3. INTEGRACIÓN Y SELECCIÓN

La sección de integración y selección la encontramos tras la de limpieza, pues muchas de las columnas que queriamos descartar han sido tratadas en la fase de limpieza para extraer información de utilidad. Hecho esto, en la presente sección procedemos a eliminar todas las columnas del dataset origial que ya no vamos a necesitar para los siguientes apartados.

```{r}
# Seleccionamos con la función "select" de dplyr todas las columnas excepto las que ya hemos limpiado para crear otras, además de aquellasque nos van a ser de poca utilidad.

booking <- booking %>%
  dplyr::select(-c("name", "search_date", "hotel_coordinates", "hotel_scores", "check.in", "check.out", "address", "features", "hotel_description", "room_data"))

# Exportamos los datos a un archivo csv
write.csv(booking, "hotel_data_processed.csv")
```

Por otro lado, hemos optado por añadir datos de vuelos por varias razones:

* Un mayor número de vuelos en un mes podría aumentar las búsquedas de hoteles cerca del aeropuerto o los precios de dichos alojamientos.
* Un mayor número de vuelos en un mes indica más turismo internacional (en lugar de sólo turismo nacional). Un tipo de turismo que podría estar buscando hoteles con características diferentes a los turistas nacionales.
* Un mayor número de vuelos en un mes indica más turismo general, lo que puede influir en las estrategias de promoción y ofertas de determinados hoteles, influyendo en última instancia en su posición en el buscador Booking y, a su vez, directamente en los precios de los hoteles.

```{r}
# Leemos los datos de vuelos
flights <- read.csv("avia_tf_apal_linear.csv.gz")
summary(flights)
```

Para añadir dicha información hacemos un tratamiento de los datos obtenidos en Eurostat a fin de obtener una media de vuelos por mes para los últimos 5 años. Una vez obtenidos, tan solo debemos hacer un merge con nuestro dataset original.

```{r}
proc_flights <- flights %>% 
        # Filtramos para quedarnos solo con los aeropuertos de interés
        # Código OACI/ICAO: Barcelona --> ES_LEBL; Valencia --> ES_LEVC; Madrid: ES_LEMD
        filter(rep_airp %in% c("ES_LEBL", "ES_LEVC", "ES_LEMD")) %>% 
        # Filtramos para quedarnos solo con los datos de carga de pasajeros
        filter(tra_meas == "PAS_CRD") %>%
        # Nos interesa solo los datos mensuales, así que filtramos por ellos
        filter(freq == "M") %>%
        separate(TIME_PERIOD, c("YEAR", "MONTH"), sep = "-") %>%
        # Nos quedamos solo con los últimos 5 años y los meses de Marzo, Junio y Diciembre
        filter(YEAR %in% c("2022", "2021", "2020", "2019", "2018") & MONTH %in% c("03", "06", "12")) %>%
        # No nos interesa los datos por aerolínea, solo los generales
        filter(airline == "TOTAL") %>%
        # Modificamos la columna de MONTH para tener los nombres del mes y cambiamos el código del aeropuerto por el nombre de la ciudad
        mutate(month_name = ifelse(MONTH == "03", "March", ifelse(MONTH == "06", "June", "December")),
               city_airp = ifelse(rep_airp == "ES_LEBL", "Barcelona", ifelse(rep_airp == "ES_LEVC", "Valencia", "Madrid"))) %>%
        # Agrupamos por ciudad y mes y calculamos la media de vuelos.
        group_by(rep_airp, month_name) %>%
        mutate(mean_flights = mean(OBS_VALUE)) %>%
        ungroup() %>%
        dplyr::select(c(city_airp, month_name, mean_flights))

# Nos quedamos con los registros únicos
final_flights <- unique(proc_flights)

# Hacemos un merge con bookings
booking <- booking %>%
  merge(final_flights, by.x = c("city", "month"), by.y = c("city_airp", "month_name"))
```


## 4. ANÁLISIS DE LOS DATOS

A continuación, se evaluará la influencia de las distintas variables sobre la posición de un hotel en el buscador y sobre el precio que este fija para sus habitaciones. Para ello se empleará una matriz de correlaciones (de Spearman, dado que no asumimos normalidad). Una vez obtenidas las variables con mayor correlación con la variable objetivo generaremos una selección de los atributos a analizar, un grupo de variables distinto por cada problema a tratar.

Tras generar los distintos grupos procederemos a evaluar la normalidad y la homogeneidad de la varianza para así poder determinar en el siguiente punto que pruebas estadísticas se pueden aplicar.

Finalmente aplicaremos distintos tests estadísticos adicionales a la correlación hallada inicialmente. Estas pruebas estadísticas incluirán contrastes de hipótesis y regresiones lineales (enfocados dese un punto de vista inferencial, no predictivo).


#### 4.1 Correlación con la variable objetivo y selección de los grupos de datos que se quieren analizar

Comenzamos evaluando la correlación (de spearman, dado que no podemos asumir normalidad y utilizamos variables categóricas también) entre las distintas variables con nuestra variable objetivo, para ello creamos una matriz de correlación y evaluamos los resultados obtenidos.

```{r}
# Transformamos las variables a numérico
booking$postal_code <- as.numeric(booking$postal_code)
booking$latitude <- as.numeric(booking$latitude)
booking$longitude <- as.numeric(booking$longitude)
booking$free_wifi <- as.numeric(booking$free_wifi)
booking$is_apartment <- as.numeric(booking$is_apartment)
booking$pet_friendly <- as.numeric(booking$pet_friendly)
booking$page_count <- booking$page_count + 1

# Seleccionamos solo las variables numéricas
booking_numeric <- booking %>%
  dplyr::select(-c("city", "month", "as.integer(!is.na(NA))"))

mattmp <- booking_numeric %>%
  dplyr::select(c("adults", "children", "num_rooms", "postal_code", "latitude", "longitude", "hotel_score", "length_description", 
           "is_suite", "is_apartment", "has_free_cancelation", "min_price","max_price", "current_page", "in_page_count", "page_count"))

sapply(mattmp, function(y) sum(is.na(y)))

# Creamos una matriz de correlación del dataset
cor_mat <- cor(mattmp, method = "spearman")
plot.new()
plot.window(xlim=c(-2,2), ylim=c(5,10))
corrplot(cor_mat, method="color", type = "lower", tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.55, addCoef.col = 'black', number.cex = 0.5)
```
```{r}
mattmp <- booking_numeric %>%
  dplyr::select(c("hotel_score", "staff_score", "facilities_score", "cleanliness_score", "comfort_score", "value_for_money_score", "location_score",
           "mean_flights", "free_wifi_score", "is_december", "is_march", "min_price", "max_price", "current_page", "in_page_count", "page_count"))

sapply(mattmp, function(y) sum(is.na(y)))

# Creamos una matriz de correlación del dataset
cor_mat <- cor(mattmp, method = "spearman")
plot.new()
plot.window(xlim=c(-2,2), ylim=c(5,10))
corrplot(cor_mat, method="color", type = "lower", tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.55, addCoef.col = 'black', number.cex = 0.5)
```

```{r}
mattmp <- booking_numeric %>%
  dplyr::select(c("balcony", "swimming_pool", "pet_friendly", "kitchen", "city_view", "is_apartment", "non_smoking_rooms", "hotel_score", 
           "private_bathroom", "elevator", "heating", "safe", "air_conditioning", "free_wifi", "max_price", "min_price", "current_page", "in_page_count", "page_count"))

sapply(mattmp, function(y) sum(is.na(y)))

# Creamos una matriz de correlación del dataset
cor_mat <- cor(mattmp, method = "spearman")
plot.new()
plot.window(xlim=c(-2,2), ylim=c(5,10))
corrplot(cor_mat, method="color", type = "lower", tl.col = "black", tl.srt = 45, diag = FALSE, tl.cex = 0.55, addCoef.col = 'black', number.cex = 0.5)
```

Como bien se puede observar en las matrices, las variables que recogen las distintas puntuaciones están correlacionadas entre ellas, por lo que un hotel que haya obtenido una puntuación elevada en un aspecto generalmente también lo obtendrá en el resto. Este factor es de especial interés si se seleccionan varias columnas de este tipo, pues corremos el riesgo de sufrir un problema de multicolinealidad. También encontramos que los hoteles con distintos atributos como aire acondicionado o baño privado suelen tener el resto de servicios adicionales, aunque este factor puede deberse a que es más habitual encontrar este tipo de elementos en una habitación de hotel, por lo que es común que cuando exista una habitación con calefacción, por ejemplo, también tenga aire acondicionado o baños privado. Por último, aunque es evidente, cabe destacar que las mayores colinealidades las tenemos con variables directamente relacionadas o que presentan información de la misma fuente (current_page con page_count o max_price con min_price). No obstante, esto no es un problema, pues eliminaremos estas variables del conjunto de entrenamiento para no incurrir en “data leakage”.

```{r}
# Filtramos para obtener cuales son las variables más correlacionadas con nuestra variable objetivo, current_page
cor_mat <- cor(booking_numeric, method = "spearman")
cor_mat_df <- as.data.frame(cor_mat)
cor_mat_df %>%
  filter(abs(page_count) > 0.065) %>%
  filter(page_count < 0.95) %>%
  arrange(page_count) %>%
  dplyr::select(page_count)
```

Cuando evaluamos la posición en la página web observamos que no hay una correlación elevada con ninguna variable, encontrando valores por debajo de 0.12 (en valor absoluto) para todos los casos. No obstante, para el caso de page_count, hemos optado por someter a estudio aquellas variables que presentan mayor correlación de spearman con la variable dependiente (pues evaluando con distintas métricas que buscan determinar el nivel de dependencia no-lineal hemos obtenido resultados similares). Las variables seleccionadas en cuestión son: kitchen, is_apartment, adults, air_conditioning, elevator, private_bathroom, num_rooms, value_for_money_score, heating, free_wifi, non_smoking_rooms, comfort_score, hotel_score, longitude, facilities_score pet_friendly, mientras que la variable dependiente será page_count.

```{r}
booking_pcount <- booking_numeric %>%
  dplyr::select(c("kitchen", "is_apartment", "adults", "air_conditioning", "elevator", "private_bathroom",
           "num_rooms", "value_for_money_score", "heating", "non_smoking_rooms", "comfort_score", "hotel_score",
           "longitude", "has_free_cancelation", "page_count"))
```

En el caso del problema a resolver relacionado con el precio, al analizar las correlaciones sí que encontramos valores entre 0.15 y 0.20. Aunque estas correlaciones no sean muy elevadas nos ofrecen un escenario más prometedor que el anteriormente presentado. A continuación, procedemos a seguir con el mismo procedimiento que con page_count, esta vez seleccionando las siguientes variables:

* **postal_code**
* **balcony**
* **pet_friendly**
* **elevator**
* **heating**
* **cleanliness_score**
* **latitude**
* **mean_flights**
* **location_score**
* **hotel_score**
* **facilities_score**
* **non_smoking_rooms**
* **comfort_score**
* **is_apartment**
* **private_bathroom**
* **kitchen**
* **air_conditioning**

```{r}

# Filtramos para obtener cuales son las variables más correlacionadas con nuestra variable objetivo, page_count
cor_mat <- cor(booking_numeric, method = "spearman")
cor_mat_df <- as.data.frame(cor_mat)
cor_mat_df %>%
  filter(abs(max_price) > 0.1) %>%
  filter(max_price < 0.95) %>%
  arrange(max_price) %>%
  dplyr::select(max_price)

booking_price <- booking_numeric %>%
  dplyr::select(c("postal_code", "balcony", "pet_friendly", "latitude", "air_conditioning", "private_bathroom", "free_wifi", "staff_score",
                  "swimming_pool", "location_score", "non_smoking_rooms", "mean_flights", "length_description", "facilities_score", "hotel_score",
                  "safe", "cleanliness_score", "comfort_score", "max_price"))
```

#### 4.2 Comprobación de la normalidad y homogeneidad de la varianza

Para comprobar si los valores de las variables de nuestro dataset se distribuyen o se aproximan a una población distribuida normalmente haremos un análisis visual, con QQ-Plots y Histogramas, y un análisis mediante el test Shapiro-Wilk. Concretamente solo comprobaremos la normalidad para las variables seleccionadas en el apartado anterior.

```{r}

norm_cols <- c("value_for_money_score", "comfort_score", "hotel_score", "staff_score", "cleanliness_score", "location_score", "facilities_score", "length_description", "min_price", "max_price", "page_count")

booking_norm <- booking_numeric %>%
  dplyr::select(norm_cols)

par(mfrow=c(2,2))
for(i in 1:ncol(booking_norm)){
  qqnorm(booking_norm[,i], main = paste("QQ-Plot de",colnames(booking_norm)[i]), col = "navy")
  qqline(booking_norm[,i], col = "red")
  hist(booking_norm[,i], main = paste("Histograma de", colnames(booking_norm)[i]), xlab=colnames(booking_norm)[i], freq = FALSE)
}
```

Los QQ-plots y los histogramas nos muestran una distribuciones con una evidente asimetría negativa en la mayoría de los casos (excepto variables como length_description que muestran una asimetría más bien positiva, aunque podría ser influencia de un dato extremo que no ha sido considerado outlier), así como un evidente problema con las colas, pues en la mayoría de los casos los puntos se alejan de la línea roja, la cual corresponde a la hipotética distribución normal. Por tanto, a priori podríamos rechazar la hipótesis de normalidad de las variables. No obstante, tras ejecutar el test Shapiro-Wilk confirmamos de forma más fiable nuestras primeras conclusiones, pues todos los p-valores muestran valores muy inferiores a 0.05, rechazando en todos los casos la hipótesis nula sobre la normamlidad de los datos.

```{r}
lapply(booking_norm, shapiro.test)
```

A fin de aproximar más los datos a una distribución normal, se propone hacer una transformación de box-cox y comprobar si los datos efectivamente se aproximan más a una normal. Tras llevar a cabo este proceso, si bien los histogramas muestran en algunos casos una distribución "visualmente más normales" y los QQ-plots parecen mostrar que la transformación ha aliviado ligeramente el problema con las colas, los resultados de la transformación no han sido suficientes como para poder considerar que los datos se distribuyen como una normal. No obstante, no debemos preocuparnos demasiado por este problema, pues tenemos suficientes registros como para poder apoyarnos en el Teorema del Límite Central. Por ejemplo, el t-test asume que las medias de las diferentes muestras se distribuyen normalmente (siendo una muestra de más de 30 registros suficiente, como rule of thumb, para que el t-test sea válido aunque los datos no se distribuyan normalmente).

```{r}
par(mfrow=c(2,3))
for(i in 1:ncol(booking_norm)){
  b <- boxcox(lm(booking_norm[,i] ~ 1))
  lambda <- b$x[which.max(b$y)]
  qqnorm((booking_norm[,i]^lambda - 1)/lambda, main = paste("QQ-Plot de",colnames(booking_norm)[i]), col = "navy")
  qqline((booking_norm[,i]^lambda - 1)/lambda, col = "red")
  hist((booking_norm[,i]^lambda - 1)/lambda, main = paste("Histograma de", colnames(booking_norm)[i]), xlab=colnames(booking_norm)[i], freq = FALSE)
}
```

Seguidamente, analizaremos la homocedasticidad mediante scatterplots y varios test de homocedasticidad, o de homogeneidad de varianzas. En este caso, no podemos utilizar el test de Levene ni el de Barlett ya que son muy sensibles ante datos no normales y, como acabamos de presentar, los datos analizados no se aproximan en ningún caso a una normal.


```{r}
ggplot(booking, aes(x = max_price, y = page_count, color = air_conditioning)) + 
  geom_point() +
  facet_grid(air_conditioning~.)
```

Como todos los test de homogeneidad de varianzas, nos basamos en las siguientes hipótesis:

$$H_0: \sigma_{i}^2 = \dots = \sigma_{j}^2$$
$$H_0: \sigma_{i}^2 \neq  \sigma_{j}^2 \textrm{ para al menos un conjunto (i,j)}$$

Donde rechazaremos la hipótesis nula cuando el pvalor del test sea inferior a un nivel de significación igual a 0.05.

```{r}
fligner.test(page_count ~ air_conditioning, data = booking)
```

```{r}
ggplot(booking, aes(x = max_price, y = page_count, color = free_wifi)) + 
  geom_point()+
  facet_grid(free_wifi~.)
```

```{r}
fligner.test(page_count ~ free_wifi, data = booking)
```

```{r}
booking <- booking %>%
  mutate(high_hotel_score = ifelse(hotel_score >= 9, 1, 0))

ggplot(booking, aes(x = max_price, y = page_count, color = high_hotel_score)) + 
  geom_point()+
  facet_grid(high_hotel_score~.)
```

```{r}
fligner.test(page_count ~ high_hotel_score, data = booking)
```

```{r}
booking <- booking %>%
  mutate(high_max_price = ifelse(max_price >= mean(max_price), 1, 0))

ggplot(booking, aes(x = min_price, y = page_count, color = high_max_price)) + 
  geom_point()+
  facet_grid(high_max_price~.)
```

```{r}
fligner.test(page_count ~ high_max_price, data = booking)
```

Tras realizar test a un gran número de variables de interés (principalmente las que van a ser sujeto de estudio para los tests estadísticos), se ha determinado que existe homogeneidad de varianzas, por lo que no nos encontraremos con problemas de heterogeneidad a la hora de realizar pruebas de hipótesis.

#### 4.3 Aplicación de pruebas estadísticas para comparar los grupos de datos
En este apartado se aplicarán distintas pruebas estadísticas para comparar los distintos grupos de datos de los que disponemos y ver cual es su relación con la variable objetivo. Esto nos permitirá evaluar que variables tienen un impacto significativo sobre la variable objetivo y cuales no.
Nuestra variable objetivo es la posición en la página (page_count) y relacionada directamente la página en la que aparece (current_page), por lo que las distintas hipótesis que se plantean están asociadas con la influencia de distintas variables sobre el posicinamiento de un hotel en Booking. A continuación se recogen unas hipótesis, aunque sería posible evaluar otras, que también pareciesen interesantes, dado el carácter académico de este trabajo nos centraremos exclusivamente en estudiar la influencia de las variables:

* **air_conditioning**
* **has_free_cancelation**
* **hotel_score**
* **min_price**

Las hipótesis que trataremos de verificar serán las siguientes:

* Aquellos hoteles con aire acondicionado tendrán una mejor posición (page_count inferior) a los que no dispongan de este.
* Los hoteles con cancelación gratuita tendrán una posición diferente a los que no dispongan de esta.
* Los hoteles con una puntuación global superior a un 9 tendrán un mejor posicionamiento.
* Los hoteles con un precio mínimo inferior a la media tendrán un mejor posicionamiento.

A continuación se estudiará cada una de estas hipótesis y el resultado obtenido.

**Hipótesis 1**
La primera hipótesis evalúa la influencia de que un hotel disponga de aire acondicionado en el posicionamiento del hotel, suponiendo que un hotel con aire acondicionado tendrá un posicionamiento mejor, es decir menor **page_count** que un hotel que no disponga de aire acondicionado. A continuación se plantea la hipótesis nula ($H_{0}$) y la alternativa ($H_{1}$).


* $H_{0}:$ La posición global de un hotel con aire acondicionado y uno sin serán iguales.
* $H_{1}:$ La posición global de un hotel con aire acondicionado será mejor (inferior) a la de uno sin.

O en otras palabras:

* $H_{0}: \mu_0 = \mu_1$
* $H_{1}: \mu_0 \le \mu_1$

En este caso se trata de un **contraste de hipótesis de dos muestras independientes sobre la media con varianzas conocidas**. Aunque previamente no se ha cumplido la normalidad de los datos por el Teorema del Límite Central podemos considerar la normalidad de los datos y aplicar un test paramétrico. En este caso aplicaremos un **test unilateral por la izquierda**.

```{r}
# Buscamos el conjunto de datos de estudio:
X1 <- booking %>%
  filter(air_conditioning==1) %>%
  dplyr::select("page_count")
X1 <- as.vector(unlist(X1),'numeric')

X2 <- booking %>%
  filter(air_conditioning==0) %>%
  dplyr::select("page_count")
X2 <- as.vector(unlist(X2),'numeric')

# Buscamos la varianza de ambas variables
var1 <- var(X1)
var2 <- var(X2)

# Definimos el nivel de confianza al 95%
alfa <- 0.05

# Buscamos las medias y la el número de registros en cada variable
mean1 <- mean(X1); n1 <- length(X1)
mean2 <- mean(X2); n2 <- length(X2)

# Calculamos el valor observado y el valor crítico
zobs <- (mean1-mean2)/sqrt(var1/n1 + var2/n2)
zcrit <- qnorm(1-alfa)

# Finalmente calculamos el pvalue
pvalue <- pnorm(zobs, lower.tail=TRUE)
print(paste("The p-value obtained is: ", pvalue))
# Mostramos las medias de las dos variables
print(paste("The mean of the group 1 is: ", mean(X1)))
print(paste("The mean of the group 2 is: ", mean(X2)))
```

Se obtiene un p-value muy cercano a 0, y muy inferior a 0.05, por lo que podemos rechazar la hipótesis nula y considerar que **la presencia de aire acondicionado en un hotel mejorará el posicionamiento de este en el buscador de Booking**. Esto se observa también al obtener una media de posicionamiento inferior en más de 9 posiciones en aquellos hoteles con aire acondicionado.

**Hipótesis 2**
Esta segunda hipótesis trata de evaluar la influencia de la cancelación gratuita, la cual para aportar una mayor diversidad en los análisis realizados, puede estar asociada a una posición distinta (inferior o superior) que la de un hotel sin cancelación gratuita. Definimos esta hipótesis mediante las hipótesis nula ($H_{0}$) y alternativa ($H_{1}$) siguientes:

* $H_{0}:$ La posición global de un hotel con cancelación gratuita y uno sin serán iguales.
* $H_{1}:$ La posición global de un hotel con cancelación gratuita será distinta a la de uno sin.

O en otras palabras:

* $H_{0}: \mu_0 = \mu_1$
* $H_{1}: \mu_0 \neq \mu_1$

En este caso se trata de un **contraste de hipótesis de dos muestras independientes sobre la media con varianzas conocidas**, al igual que en el caso anterior por el TLC podemos aceptar la normalidad de los datos y aplicar un test paramétrico, en concreto un **test bilateral**.

```{r}
# Buscamos el conjunto de datos de estudio:
X1 <- booking %>%
  filter(has_free_cancelation==1) %>%
  dplyr::select("page_count")
X1 <- as.vector(unlist(X1),'numeric')

X2 <- booking %>%
  filter(has_free_cancelation==0) %>%
  dplyr::select("page_count")
X2 <- as.vector(unlist(X2),'numeric')

# Buscamos la varianza de ambas variables
var1 <- var(X1)
var2 <- var(X2)

# Definimos el nivel de confianza al 95%
alfa <- 0.05

# Buscamos las medias y la el número de registros en cada variable
mean1 <- mean(X1); n1 <- length(X1)
mean2 <- mean(X2); n2 <- length(X2)

# Calculamos el valor observado y el valor crítico
zobs <- (mean1-mean2)/sqrt(var1/n1 + var2/n2)
zcrit <- qnorm(1-(alfa/2))

# Finalmente calculamos el pvalue
pvalue <- pnorm(zobs, lower.tail=TRUE)
print(paste("The p-value obtained is: ", pvalue))
# Mostramos las medias de las dos variables
print(paste("The mean of the group 1 is: ", mean(X1)))
print(paste("The mean of the group 2 is: ", mean(X2)))
```

Observamos que se obtiene un p-value inferior a 0.05 por lo que debemos rechazar la hipótesis nula y, por lo tanto, obtenemos que **un hotel con cancelación gratuita obtendrá un posicionamiento distinto al de un hotel sin cancelación gratuita**. En concreto se ha observado que los hoteles con cancelación gratuita obtienen un mejor posicionamiento que aquellos sin.

**Hipótesis 3**
A continuación evaluamos la influencia de la puntuación global del hotel. En este caso evaluaremos si un hotel con una puntuación global (**hotel_score**) superior o igual a un 9.0 obtiene un mejor posicionamiento que un hotel con una peor puntuación. A continuación se plantea la hipótesis nula ($H_{0}$) y la alternativa ($H_{1}$)

Los hoteles con una puntuación global superior a un 9 tendrán un mejor posicionamiento.

* $H_{0}:$ La posición global de un hotel con una puntuación global mayor o igual a 9.0 y uno con una puntuación inferior serán iguales.
* $H_{1}:$ La posición global de un hotel con una puntuación global mayor o igual a 9.0 será mejor (menor) que uno con una puntuación inferior.

O en otras palabras:

* $H_{0}: \mu_0 = \mu_1$
* $H_{1}: \mu_0 \le \mu_1$

En este caso se trata de un **contraste de hipótesis de dos muestras independientes sobre la media con varianzas conocidas**. Al igual que en los casos anteriores podemos asumir la normalidad de los datos por el TLC. En este caso se aplicará un **test bilateral por la izquierda**.

```{r}
# Buscamos el conjunto de datos de estudio:
X1 <- booking %>%
  filter(hotel_score>=9.0) %>%
  dplyr::select("page_count")
X1 <- as.vector(unlist(X1),'numeric')

X2 <- booking %>%
  filter(hotel_score<9.0) %>%
  dplyr::select("page_count")
X2 <- as.vector(unlist(X2),'numeric')

# Buscamos la varianza de ambas variables
var1 <- var(X1)
var2 <- var(X2)

# Definimos el nivel de confianza al 95%
alfa <- 0.05

# Buscamos las medias y la el número de registros en cada variable
mean1 <- mean(X1); n1 <- length(X1)
mean2 <- mean(X2); n2 <- length(X2)

# Calculamos el valor observado y el valor crítico
zobs <- (mean1-mean2)/sqrt(var1/n1 + var2/n2)
zcrit <- qnorm(1-alfa)

# Finalmente calculamos el pvalue
pvalue <- pnorm(zobs, lower.tail=TRUE)
print(paste("The p-value obtained is: ", pvalue))
# Mostramos las medias de las dos variables
print(paste("The mean of the group 1 is: ", mean(X1)))
print(paste("The mean of the group 2 is: ", mean(X2)))
```

Se obtiene un valor del p-value de 0.048, el cual aún siendo muy cercano a 0.05 que utilizamos para el intérvalo de confianza es inferior, por lo que se debe rechazar la hipótesis nula y podemos decir que **un hotel con una puntuación superior a 9.0 obtendrá un mejor posicionamiento que el resto de hoteles**. No obstante, por el valor obtenido podemos determinar que esta variable no es tan influyente como el aire acondicionado o la cancelación gratuita.

**Hipótesis 4**
Finalmente evaluamos la influencia del precio mínimo del hotel considerando que un hotel con un menor precio mínimo tendrá un mejor posicionamiento. Para ello consideraremos las siguientes hipótesis nula ($H_{0}$) y alternativa ($H_{1}$)

Los hoteles con un precio mínimo inferior a la media tendrán un mejor posicionamiento.

* $H_{0}:$ La posición global de un hotel con precio mínimo inferior a la media y uno superior serán iguales.
* $H_{1}:$ La posición global de un hotel con precio mínimo inferior a la media será mejor (inferior) a la de uno con precio superior.

O en otras palabras:

* $H_{0}: \mu_0 = \mu_1$
* $H_{1}: \mu_0 \le \mu_1$

En este caso también realizaremos un **contraste de hipótesis de dos muestras independientes sobre la media con varianzas conocidas**. Como se ha visto en los casos anteriores, por el TLC podemos asumir la normalidad de los datos y aplicaremos un test paramétrico, en este caso emplearemos un **test unilateral por la izquierda**.

```{r}
# Buscamos el conjunto de datos de estudio:
X1 <- booking %>%
  filter(min_price<mean(min_price)) %>%
  dplyr::select("page_count")
X1 <- as.vector(unlist(X1),'numeric')

X2 <- booking %>%
  filter(min_price>=mean(min_price)) %>%
  dplyr::select("page_count")
X2 <- as.vector(unlist(X2),'numeric')

# Buscamos la varianza de ambas variables
var1 <- var(X1)
var2 <- var(X2)

# Definimos el nivel de confianza al 95%
alfa <- 0.05

# Buscamos las medias y la el número de registros en cada variable
mean1 <- mean(X1); n1 <- length(X1)
mean2 <- mean(X2); n2 <- length(X2)

# Calculamos el valor observado y el valor crítico
zobs <- (mean1-mean2)/sqrt(var1/n1 + var2/n2)
zcrit <- qnorm(1-alfa)

# Finalmente calculamos el pvalue
pvalue <- pnorm(zobs, lower.tail=TRUE)
print(paste("The p-value obtained is: ", pvalue))
# Mostramos las medias de las dos variables
print(paste("The mean of the group 1 is: ", mean(X1)))
print(paste("The mean of the group 2 is: ", mean(X2)))
```

En este caso se ha obtenido un p-value de 0.99, por lo que se acepta la hipótesis nula y obtenemos que **los hoteles con un precio mínimo inferior a la media no obtiene un mejor posicionamiento que el resto de hoteles**. Es más en este caso se ha observado que que la media de posicionamiento de los hoteles con un precio mínimo superior a la media ha sido ligeramente mejor al resto.

#### 4.4 Aplicación de funciones lineales para resolución del problema
Una vez analizados los distintos contrastes de hipótesis y hayada la correlación entre las distintas variables, tenemos un conocimiento del conjunto de datos lo suficientemente extenso como para analizar mediante una función lineal el posicionamiento de un hotel en el buscador de Booking.

Tras los resultados que se han obtenido observamos que hay distintas variables que pueden tener una mayor influencia sobre nuestra variable objetivo. Por un lado, si nos basamos en lo obtenido tras el análisis de correlación hayamos que variables como **min_price**, **comfort_score**, **cleanliness_score**, **safe** o **hotel_score** entre otros tienen una gran relevancia. Por otro lado, variables como **air_conditioning** o **has_free_cancelation** han mostrado una relevancia significativa durante el contraste de hipótesis. 

En este apartado comenzaremos tratando de ajustar una regresión lineal simple aproximando la variable dependiente **page_count** con la variable explicativa que ha proporcionado una mayor correlación, que ha resultado ser **comfort_score**.

```{r}
#Create the linear model
mrl <- lm(page_count~C, data=booking)

#Evaluate the data
summary(mrl)
```

Obtenemos un valor de R-squared de 0.005, lo que es un valor muy bajo y representa una aproximación muy mala de la variable dependiente.

Con el objetivo de mejorar los resultados obtenidos se realizará una regresión lineal múltiple que consideré todas las puntuaciones del hotel para predecir la posición de este. Para ello empleamos las variables: **hotel_score**, **staff_score**, **facilities_score**, **cleanliness_score**, **comfort_score**, **value_for_money_score**, **location_score** y **free_wifi_score**, para ver como la opinión de los clientes afecta en el posicionamiento de este en Booking.

```{r}
#Create the linear model
mrl_scores <- lm(page_count~hotel_score + staff_score + facilities_score + cleanliness_score + comfort_score + value_for_money_score + location_score + free_wifi_score, data=booking)

#Evaluate the data
summary(mrl_scores)
```

Se observa que con la regresión lineal multiple R-squared aumenta hasta 0.02, unas 4 veces mejor que empleando solo una variable, pero aún así los resultados obtenidos son muy malos y no aproximan de una forma correcta la variable dependiente.

De este primer análisis observamos que de todas las variables empleadas, las que han tenido una mayor influencia sobre la variable dependiente han sido **staff_score**, **value_for_money_score**, **location_store** y **free_wifi_score**. A continuación generamos una regresión lineal múltiple empleando tan solo estas variables.


```{r}
#Create the linear model
mrl_scores <- lm(page_count~staff_score + value_for_money_score + location_score + free_wifi_score, data=booking)

#Evaluate the data
summary(mrl_scores)
```

Observamos que habiendo empleado tan solo 4 variables, de las 8 empleadas previamente, el R-squared tan solo se ha visto reducido en 0.0015.

A continuación, mantendremos estas variables que han mostrado los mejores resultados y añadiremos las que habíamos hallado que tenían una mayor correlación con la variable dependiente.

```{r}
#Create the linear model
mrl_complete <- lm(page_count~staff_score + value_for_money_score + location_score + free_wifi_score + min_price + balcony + pet_friendly + air_conditioning + private_bathroom + free_wifi + swimming_pool + non_smoking_rooms + mean_flights + length_description + safe, data=booking)

#Evaluate the data
summary(mrl_complete)
```

Con todas las variables la R-squared ha aumentado hasta 0.057, que sigue siendo un resultado muy bajo, pero dificil de mejorar con unos datos tan poco correlacionados con la variable objetivo.

De todas las variables empleadas se observa que las que tienen una mayor influencia son: **staff_score**, **value_for_money_score**, **free_wifi_score**, **balcony**, **pet_friendly**, **air_conditioning**, **swimming_pool**, **mean_flights** y **length_description**. A continuación se aproxima la variable dependiente basándonos exclusivamente en estas variables.

```{r}
#Create the linear model
mrl_final <- lm(page_count~staff_score + value_for_money_score + free_wifi_score + balcony + pet_friendly + air_conditioning + swimming_pool + mean_flights + length_description, data=booking)

#Evaluate the data
summary(mrl_final)
```

De esta forma hemos conseguido mantener una R-squared similar a la obtenida previamente, pero empleando exclusivamente 9 variables en lugar de las más de 40 variables iniciales.


Anteriormente se ha observado una correlación entre el precio de la habitación y sus características, en concreto había una correlación con el precio máximo de esta. A continuación, se tratará de predecir el precio máximo de la habitación mediante una regresión lineal múltiple empleando el resto de variables para ello:

```{r}
#Create the linear model
mrl_price <- lm(max_price~staff_score + value_for_money_score + location_score + free_wifi_score + balcony + pet_friendly + air_conditioning + private_bathroom + free_wifi + swimming_pool + non_smoking_rooms + mean_flights + length_description + safe, data=booking)

#Evaluate the data
summary(mrl_price)
```

Con ello obtenemos una R-squared de 0.2125, con lo que podemos observar que variables que tienen una mayor correlación con la variable objetivo nos proporcionarán unos resultados más apropiados. A portir de los resultados mostrados podemos mejorar la regresión empleando tan solo aquellas variables que han aportado más información en la regresión como han sido: **staff_score**, **location_score**, **balcony**, **air_conditioning**, **free_wifi**, **swimming_pool**, **non_smoking_rooms**, **mean_flights**, **length_description** y **safe**.

A continuación se emplea una regresión lineal múltiple tan solo con estas variables.

```{r}
#Create the linear model
mrl_price <- lm(max_price~staff_score + location_score + balcony + air_conditioning + free_wifi + swimming_pool + non_smoking_rooms + mean_flights + length_description + safe, data=booking)

#Evaluate the data
summary(mrl_price)
```

Se observa que aun habiendo reducido el número de variables, mantenemos una R-squares ligeramente superior 0.21. Hay que tener en cuenta que los resultados siguen sin ser satisfactorios, ya que una R-squared de 0.21 es bastante baja, no obstante es mucho mejor que lo obtenido tratando de predecir la variable page_count. Esta mala predicción es producida como se había comentado previamente debido a la baja correlación de las variables con la variable objetivo.

## 5. REPRESENTACIÓN DE LOS RESULTADOS

A lo largo del documento se han presentado representaciones gráficas de los análisis realizados y de los resultados obtenidos.

## 6. RESOLUCIÓN DEL PROBLEMA
